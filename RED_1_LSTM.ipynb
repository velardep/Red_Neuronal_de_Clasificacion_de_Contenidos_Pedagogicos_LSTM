{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMu+5c0MVHRPusa/PzILUoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/velardep/Red_Neuronal-_de-_Clasificaci-n-_de-_Contenidos-_Pedag-gicos-_-LSTM-/blob/main/RED_1_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"DEVICE:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nKIsyNqgmBy",
        "outputId": "cac3edce-1a70-41f0-9869-2b6e97500600"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.9.0+cu126\n",
            "cuda available: True\n",
            "DEVICE: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install transformers scikit-learn pandas numpy tqdm pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ciWoaxhvFA",
        "outputId": "642b27ea-2dbc-4bf8-b657-501dfb81afe3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, random, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, classification_report\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", DEVICE, \"| torch:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqVwpF78hwJ9",
        "outputId": "c251f573-bfe6-4eea-9654-97a48d66161b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda | torch: 2.9.0+cu126\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 3: Carga PDFs (Secundaria + Primaria) y extrae texto (PDFs no escaneados)\n",
        "\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Nombres exactos de los archivos que subiste\n",
        "PDF_SEC = \"Prontuario de mis aprendizajes - Educación Secundaria Comunitaria Productiva.pdf\"\n",
        "PDF_PRI = \"Prontuario de mis aprendizajes - Educación Primaria Comunitaria Vocacional.pdf\"\n",
        "\n",
        "# Función para buscar PDF en rutas típicas\n",
        "def find_pdf(pdf_name):\n",
        "    candidate_paths = [\n",
        "        f\"/content/{pdf_name}\",\n",
        "        f\"/mnt/data/{pdf_name}\",\n",
        "        pdf_name,  # por si está en el cwd\n",
        "    ]\n",
        "    for p in candidate_paths:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\n",
        "        f\"No encontré el PDF. Sube el archivo con el mismo nombre: {pdf_name} \"\n",
        "        f\"y verifica que esté en /content. Paths intentados: {candidate_paths}\"\n",
        "    )\n",
        "\n",
        "# Buscar y leer PDFs\n",
        "sec_path = find_pdf(PDF_SEC)\n",
        "pri_path = find_pdf(PDF_PRI)\n",
        "\n",
        "print(\"PDF Secundaria encontrado en:\", sec_path)\n",
        "print(\"PDF Primaria encontrado en:\", pri_path)\n",
        "\n",
        "# Función para extraer texto de un PDF\n",
        "def extract_text_from_pdf(path):\n",
        "    reader = PdfReader(path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += (page.extract_text() or \"\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Extraer textos\n",
        "full_text_sec = extract_text_from_pdf(sec_path)\n",
        "full_text_pri = extract_text_from_pdf(pri_path)\n",
        "\n",
        "# Combinar textos\n",
        "full_text = full_text_sec + \"\\n\" + full_text_pri\n",
        "\n",
        "print(\"Caracteres totales combinados extraídos:\", len(full_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUYKXpR_-O57",
        "outputId": "f687e863-deb1-4d95-9760-3bcb51cba0bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF Secundaria encontrado en: /content/Prontuario de mis aprendizajes - Educación Secundaria Comunitaria Productiva.pdf\n",
            "PDF Primaria encontrado en: /content/Prontuario de mis aprendizajes - Educación Primaria Comunitaria Vocacional.pdf\n",
            "Caracteres totales combinados extraídos: 2430066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 4: Limpieza y división en chunks robustos (para PDFs largos)\n",
        "def clean_text(text: str) -> str:\n",
        "    text = text.replace(\"\\x00\", \" \")\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text(text: str, max_words: int = 260, overlap_words: int = 60):\n",
        "    # Divide por palabras con overlap para no perder contexto en límites\n",
        "    words = text.split()\n",
        "    if len(words) < 30:\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + max_words, len(words))\n",
        "        c = \" \".join(words[start:end]).strip()\n",
        "        if len(c) >= 120:  # filtro mínimo para evitar basura\n",
        "            chunks.append(c)\n",
        "        if end == len(words):\n",
        "            break\n",
        "        start = max(0, end - overlap_words)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "full_text = clean_text(full_text)\n",
        "chunks = chunk_text(full_text, max_words=260, overlap_words=60)\n",
        "\n",
        "print(\"Total de chunks:\", len(chunks))\n",
        "print(\"Ejemplo de chunk (primeros 350 chars):\\n\", chunks[0][:350])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOt5Ft-Q-bPB",
        "outputId": "44cfe8b5-3fef-4314-cf72-2097f322c004"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de chunks: 1874\n",
            "Ejemplo de chunk (primeros 350 chars):\n",
            " Prontuario de mis aprendizajes Educación Secundaria Comunitaria Productiva Omar Veliz Ramos MINISTRO DE EDUCACIÓN Manuel Eudal Tejerina del Castillo VICEMINISTRO DE EDUCACIÓN REGULAR Delia Yucra Rodas DIRECTORA GENERAL DE EDUCACIÓN SECUNDARIA Equipo de redacción Dirección General de Educación Secundaria Revisión Instituto de Investigaciones Pedagóg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 5: Define etiquetas finales (ÁREA x DIMENSIÓN)\n",
        "# Reglas:\n",
        "# - Áreas/Componentes típicos en secundaria (ajustable luego, pero estas quedan fijas en el modelo)\n",
        "# - Dimensiones: SABER / HACER / SER / DECIDIR\n",
        "# - Etiqueta final = \"{AREA}__{DIMENSION}\"  -> multi-label\n",
        "\n",
        "AREAS = [\n",
        "    \"MATEMATICA\",\n",
        "    \"COMUNICACION_Y_LENGUAJES\",\n",
        "    \"CIENCIAS_NATURALES_BIOLOGIA\",\n",
        "    \"FISICA\",\n",
        "    \"QUIMICA\",\n",
        "    \"CIENCIAS_SOCIALES\",\n",
        "    \"VALORES_ESPIRITUALIDAD_RELIGIONES\",\n",
        "    \"TECNOLOGIA_PRODUCTIVA\",\n",
        "]\n",
        "\n",
        "DIMENSIONES = [\"SABER\", \"HACER\", \"SER\", \"DECIDIR\"]\n",
        "\n",
        "LABELS = [f\"{a}__{d}\" for a in AREAS for d in DIMENSIONES]\n",
        "\n",
        "print(\"Total de etiquetas:\", len(LABELS))\n",
        "print(\"Primeras 12 etiquetas:\", LABELS[:12])\n",
        "print(\"Últimas 12 etiquetas:\", LABELS[-12:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkjHJ52q-hxM",
        "outputId": "e24d2a71-f979-4b68-ef19-0c085afacc63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de etiquetas: 32\n",
            "Primeras 12 etiquetas: ['MATEMATICA__SABER', 'MATEMATICA__HACER', 'MATEMATICA__SER', 'MATEMATICA__DECIDIR', 'COMUNICACION_Y_LENGUAJES__SABER', 'COMUNICACION_Y_LENGUAJES__HACER', 'COMUNICACION_Y_LENGUAJES__SER', 'COMUNICACION_Y_LENGUAJES__DECIDIR', 'CIENCIAS_NATURALES_BIOLOGIA__SABER', 'CIENCIAS_NATURALES_BIOLOGIA__HACER', 'CIENCIAS_NATURALES_BIOLOGIA__SER', 'CIENCIAS_NATURALES_BIOLOGIA__DECIDIR']\n",
            "Últimas 12 etiquetas: ['CIENCIAS_SOCIALES__SABER', 'CIENCIAS_SOCIALES__HACER', 'CIENCIAS_SOCIALES__SER', 'CIENCIAS_SOCIALES__DECIDIR', 'VALORES_ESPIRITUALIDAD_RELIGIONES__SABER', 'VALORES_ESPIRITUALIDAD_RELIGIONES__HACER', 'VALORES_ESPIRITUALIDAD_RELIGIONES__SER', 'VALORES_ESPIRITUALIDAD_RELIGIONES__DECIDIR', 'TECNOLOGIA_PRODUCTIVA__SABER', 'TECNOLOGIA_PRODUCTIVA__HACER', 'TECNOLOGIA_PRODUCTIVA__SER', 'TECNOLOGIA_PRODUCTIVA__DECIDIR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 6: Auto-etiquetado determinista (reglas) para poder ENTRENAR hoy con tu PDF\n",
        "# IMPORTANTE:\n",
        "# - Esto NO es \"simulación aleatoria\".\n",
        "# - Es weak-supervision: reglas reproducibles basadas en palabras-clave.\n",
        "# - En producción, puedes mejorar etiquetado con validación docente (active learning) sin cambiar arquitectura.\n",
        "\n",
        "AREA_KEYWORDS = {\n",
        "    \"MATEMATICA\": [\n",
        "        # Conceptos generales y Aritmética\n",
        "        \"números y operaciones\", \"sistemas de numeración\", \"conjuntos numéricos\",\n",
        "        \"números naturales\", \"números racionales\", \"números reales\",\n",
        "        \"números irracionales\", \"números primos\", \"números decimales\",\n",
        "        \"número entero\", \"numeración binaria\", \"razones\", \"proporciones\",\n",
        "        \"regla de tres\", \"análisis numérico\", \"lógica matemática\",\n",
        "        # Álgebra y Cálculo\n",
        "        \"álgebra\", \"expresiones algebraicas\", \"ecuaciones de primer grado\",\n",
        "        \"ecuaciones cuadráticas\", \"sistemas de ecuaciones\", \"desigualdades\",\n",
        "        \"inecuaciones\", \"factorización\", \"fracciones algebraicas\",\n",
        "        \"exponentes\", \"radicales\", \"números complejos\", \"sucesiones\",\n",
        "        \"progresiones\", \"análisis matemático\", \"cálculo infinitesimal\",\n",
        "        \"límites\", \"continuidad\", \"derivada\", \"integrales\", \"álgebra lineal\",\n",
        "        # Geometría y Trigonometría\n",
        "        \"geometría\", \"plano cartesiano\", \"triángulos\", \"semejanza\",\n",
        "        \"áreas\", \"perímetros\", \"formas tridimensionales\", \"geometría analítica\",\n",
        "        \"línea recta\", \"circunferencia\", \"parábola\", \"elipse\", \"hipérbola\",\n",
        "        \"trigonometría\", \"identidades trigonométricas\", \"análisis vectorial\",\n",
        "        # Estadística y Otros\n",
        "        \"estadística\", \"probabilidad\", \"combinatoria\", \"matemática financiera\",\n",
        "        \"teoría de conjuntos\", \"automatización\", \"inteligencia artificial\"\n",
        "    ],\n",
        "    \"COMUNICACION_Y_LENGUAJES\": [\n",
        "        # Lingüística y Gramática\n",
        "        \"lenguaje oral\", \"lenguaje escrito\", \"signos sonoros\", \"fonología\",\n",
        "        \"fonética\", \"habla\", \"morfología gramatical\", \"verbos regulares\",\n",
        "        \"sustantivos\", \"adjetivos\", \"neologismos\", \"palabras homógrafas\",\n",
        "        \"palabras multiformes\", \"polisémicas\", \"definiciones\", \"mayúsculas\",\n",
        "        # Tipos de texto y Comunicación\n",
        "        \"medios de comunicación social\", \"masas\", \"géneros periodísticos\",\n",
        "        \"textos orales\", \"cómic\", \"comunicación efectiva\", \"monografía\",\n",
        "        \"investigación\", \"referencias bibliográficas\", \"justificación\",\n",
        "        \"objetivos\", \"mapas mentales\", \"mapa conceptual\", \"trabalenguas\"\n",
        "    ],\n",
        "    \"CIENCIAS_NATURALES_BIOLOGIA\": [\n",
        "        # Ramas de la Biología\n",
        "        \"biología\", \"botánica\", \"zoología\", \"ecología\", \"microbiología\",\n",
        "        \"virología\", \"genética\", \"entomología\", \"ornitología\", \"herpetología\",\n",
        "        \"bacteriología\", \"protozoología\", \"pteridología\", \"ficología\",\n",
        "        \"ictiología\", \"biomatemáticas\", \"biogeografía\", \"bioquímica\",\n",
        "        \"biofísica\", \"bioética\",\n",
        "        # Conceptos Biológicos\n",
        "        \"vida\", \"organismo\", \"célula\", \"moléculas orgánicas\", \"nutrientes\",\n",
        "        \"procesos metabólicos\", \"evolución\", \"crecimiento\", \"origen\",\n",
        "        \"distribución\", \"ecosistemas\", \"naturaleza\"\n",
        "    ],\n",
        "    \"FISICA\": [\n",
        "        # Ramas de la Física\n",
        "        \"física clásica\", \"física moderna\", \"física contemporánea\", \"mecánica\",\n",
        "        \"termodinámica\", \"movimiento ondulatorio\", \"óptica\", \"electromagnetismo\",\n",
        "        \"relatividad\", \"mecánica cuántica\", \"física de partículas\", \"gravitación\",\n",
        "        \"dinámica no lineal\", \"sistemas complejos\", \"nanofísica\",\n",
        "        # Conceptos y Mediciones\n",
        "        \"materia\", \"energía\", \"fenómenos naturales\", \"mediciones\", \"magnitudes\",\n",
        "        \"cantidad patrón\", \"cinemática\", \"dinámica\", \"velocidad\", \"tiempo\",\n",
        "        \"espacio\", \"vector\", \"campo vectorial\", \"fuerza\"\n",
        "    ],\n",
        "    \"QUIMICA\": [\n",
        "        # General y Ramas\n",
        "        \"química general\", \"estructura de la materia\", \"composición\",\n",
        "        \"transformaciones\", \"elementos naturales\",\n",
        "        # Aplicaciones y Contexto\n",
        "        \"fármacos\", \"plaguicidas\", \"armas químicas\", \"compuestos\",\n",
        "        \"análisis empírico\", \"contexto inorgánico\"\n",
        "    ],\n",
        "    \"CIENCIAS_SOCIALES\": [\n",
        "        # Disciplinas\n",
        "        \"historia\", \"antropología\", \"sociología\", \"ciencia política\",\n",
        "        \"economía\", \"lingüística\", \"psicología\", \"derecho\", \"geografía\",\n",
        "        \"demografía\", \"arqueología\", \"ciencias de la educación\",\n",
        "        # Conceptos Políticos y Sociales\n",
        "        \"estado plurinacional\", \"constitución política\", \"democracia\",\n",
        "        \"gobierno\", \"poder político\", \"conflictos sociales\", \"instituciones\",\n",
        "        \"territorialidad\", \"función pública\", \"ley 450\", \"vulnerabilidad\",\n",
        "        \"naciones y pueblos indígena originarios\", \"aislamiento voluntario\",\n",
        "        # Economía\n",
        "        \"producción\", \"distribución\", \"consumo\", \"bienes y servicios\",\n",
        "        \"producto interno bruto (pib)\", \"desarrollo económico\", \"comercio internacional\"\n",
        "    ],\n",
        "    \"VALORES_ESPIRITUALIDAD_RELIGIONES\": [\n",
        "        # Principios Andinos y Constitucionales\n",
        "        \"vivir bien\", \"suma qamaña\", \"ñandereko\", \"teko kavi\", \"ivi maraei\",\n",
        "        \"qhapaj ñan\", \"ama qhilla\", \"ama llulla\", \"ama suwa\",\n",
        "        # Valores Éticos\n",
        "        \"unidad\", \"igualdad\", \"inclusión\", \"dignidad\", \"libertad\",\n",
        "        \"solidaridad\", \"reciprocidad\", \"respeto\", \"complementariedad\",\n",
        "        \"armonía\", \"transparencia\", \"equilibrio\", \"bienestar común\",\n",
        "        \"responsabilidad\", \"justicia social\", \"vida armoniosa\"\n",
        "    ],\n",
        "\n",
        "    \"TECNOLOGIA_PRODUCTIVA\": [\n",
        "        \"inteligencia artificial\", \"programación\", \"automatización\", \"robot\",\n",
        "        \"redes neuronales\", \"chatbot\", \"proyecto\", \"plan sectorial\",\n",
        "        \"necesidad\", \"solución de problemas\", \"intervención\", \"investigación\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "DIM_KEYWORDS = {\n",
        "    \"SABER\": [\n",
        "        \"definir\", \"reconocer\", \"identificar\", \"conocer\", \"describir\",\n",
        "        \"explicar\", \"comprender\", \"interpretar\", \"analizar\", \"estudiar\",\n",
        "        \"adquirir conocimientos\", \"profundizar\", \"entender\", \"teorizar\"\n",
        "    ],\n",
        "    \"HACER\": [\n",
        "        \"resolver\", \"aplicar\", \"elaborar\", \"construir\", \"desarrollar\",\n",
        "        \"realizar\", \"calcular\", \"producir\", \"experimentar\", \"investigar\",\n",
        "        \"redactar\", \"medir\", \"comparar\", \"transformar\", \"ejecutar\",\n",
        "        \"diseñar\", \"practicar\", \"utilizar\"\n",
        "    ],\n",
        "    \"SER\": [\n",
        "        \"valorar\", \"respetar\", \"asumir\", \"convivir\", \"colaborar\",\n",
        "        \"responsable\", \"solidario\", \"ético\", \"actitud\", \"reciprocidad\",\n",
        "        \"complementariedad\", \"integridad\", \"consciente\", \"dignidad\",\n",
        "        \"inclusión\", \"equidad\"\n",
        "    ],\n",
        "    \"DECIDIR\": [\n",
        "        \"decidir\", \"elegir\", \"proponer\", \"planificar\", \"argumentar\",\n",
        "        \"evaluar\", \"seleccionar\", \"priorizar\", \"comprometer\", \"promover\",\n",
        "        \"transformar la realidad\", \"satisfacer necesidades\", \"vincular\",\n",
        "        \"integrar\", \"participar\", \"solucionar\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "MIN_AREA_HITS = 2      # exige evidencia para asignar área\n",
        "MAX_DIMS = 2           # evita explotar etiquetas\n",
        "FALLBACK = \"CIENCIAS_SOCIALES__SABER\"\n",
        "\n",
        "def label_area(text: str):\n",
        "    areas = infer_areas(text)          # tu función\n",
        "    return areas[0] if areas else None\n",
        "\n",
        "def label_dims(text: str):\n",
        "    dims = infer_dims(text)            # tu función\n",
        "    return dims if dims else None\n",
        "\n",
        "rows = []\n",
        "for c in tqdm(chunks, desc=\"Etiquetando chunks\"):\n",
        "    a = label_area(c)\n",
        "    d = label_dims(c)\n",
        "    if a is None or d is None:\n",
        "        continue  # fuera basura\n",
        "    rows.append({\"text\": c, \"area\": a, \"dims\": d})\n",
        "\n",
        "df = pd.DataFrame(rows).reset_index(drop=True)\n",
        "print(df.shape)\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FGzDpQiS-pze",
        "outputId": "ae88cbd8-ff5e-4b18-80f0-faa9be7ae244"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Etiquetando chunks: 100%|██████████| 1874/1874 [00:01<00:00, 1127.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(995, 3)\n",
            "                                                text  \\\n",
            "0  Prontuario de mis aprendizajes Educación Secun...   \n",
            "1  general DE MIS APRENDIZAJES Geometría analític...   \n",
            "2  Óxidos ácidos o anhídridos ......................   \n",
            "3  583 Articulaciones ..............................   \n",
            "4  lenguajes Comunicación La comunicación ..........   \n",
            "\n",
            "                          area              dims  \n",
            "0                   MATEMATICA           [SABER]  \n",
            "1                   MATEMATICA           [SABER]  \n",
            "2  CIENCIAS_NATURALES_BIOLOGIA           [SABER]  \n",
            "3     COMUNICACION_Y_LENGUAJES           [SABER]  \n",
            "4     COMUNICACION_Y_LENGUAJES  [HACER, DECIDIR]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7\n",
        "\n",
        "TOKENIZER_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Área (single label)\n",
        "area_to_id = {a:i for i,a in enumerate(AREAS)}\n",
        "y_area = np.array([area_to_id[a] for a in df[\"area\"].tolist()])\n",
        "\n",
        "# Dimensiones (multi-label 4)\n",
        "mlb_dim = MultiLabelBinarizer(classes=DIMENSIONES)\n",
        "y_dim = mlb_dim.fit_transform(df[\"dims\"].values)\n",
        "\n",
        "X_train, X_val, ya_train, ya_val, yd_train, yd_val = train_test_split(\n",
        "    df[\"text\"].tolist(), y_area, y_dim, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "enc_train = tokenizer(X_train, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "enc_val   = tokenizer(X_val,   truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "\n",
        "class DualDataset(Dataset):\n",
        "    def __init__(self, enc, ya, yd):\n",
        "        self.ids = enc[\"input_ids\"]\n",
        "        self.att = enc[\"attention_mask\"]\n",
        "        self.ya = torch.tensor(ya).long()\n",
        "        self.yd = torch.tensor(yd).float()\n",
        "\n",
        "    def __len__(self): return self.ids.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.ids[idx], \"attention_mask\": self.att[idx], \"y_area\": self.ya[idx], \"y_dim\": self.yd[idx]}\n",
        "\n",
        "train_ds = DualDataset(enc_train, ya_train, yd_train)\n",
        "val_ds   = DualDataset(enc_val,   ya_val,   yd_val)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2GNXjePi4Q1",
        "outputId": "adc9b88d-01e4-4393-f371-e1114ddf59e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 796 Val: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 8: Modelo LSTM multi-head + entrenamiento robusto (SER/DECIDIR raras)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "PAD_ID = tokenizer.pad_token_id\n",
        "NUM_AREAS = len(AREAS)\n",
        "NUM_DIMS  = len(DIMENSIONES)\n",
        "\n",
        "EMB_DIM = 256\n",
        "HIDDEN = 256\n",
        "\n",
        "class MultiHeadPedagogicalLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, pad_id, emb_dim, hidden_dim, num_areas, num_dims):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc_area = nn.Linear(hidden_dim * 2, num_areas)  # softmax\n",
        "        self.fc_dim  = nn.Linear(hidden_dim * 2, num_dims)   # sigmoid multi-label\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids)      # [B,T,E]\n",
        "        out, _ = self.lstm(x)              # [B,T,2H]\n",
        "\n",
        "        mask = attention_mask.unsqueeze(-1).float()  # [B,T,1]\n",
        "        out = out * mask\n",
        "        pooled = out.sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)  # [B,2H]\n",
        "\n",
        "        pooled = self.dropout(pooled)\n",
        "\n",
        "        logits_area = self.fc_area(pooled)  # [B,num_areas]\n",
        "        logits_dim  = self.fc_dim(pooled)   # [B,num_dims]\n",
        "        return logits_area, logits_dim\n",
        "\n",
        "model = MultiHeadPedagogicalLSTM(VOCAB_SIZE, PAD_ID, EMB_DIM, HIDDEN, NUM_AREAS, NUM_DIMS).to(DEVICE)\n",
        "\n",
        "# ✅ PESOS para ÁREA (desbalance)\n",
        "area_counts = np.bincount(ya_train, minlength=NUM_AREAS).astype(np.float32)\n",
        "area_weights = (area_counts.sum() / (area_counts + 1e-6))\n",
        "area_weights = torch.tensor(area_weights, dtype=torch.float32).to(DEVICE)\n",
        "loss_area = nn.CrossEntropyLoss(weight=area_weights)\n",
        "\n",
        "# ✅ pos_weight + Focal para DIM (SER/DECIDIR raras)\n",
        "pos = yd_train.sum(axis=0)                 # positivos por dim\n",
        "neg = yd_train.shape[0] - pos\n",
        "pos_weight_dim = torch.tensor(neg / (pos + 1e-6), dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "class FocalBCEWithLogitsLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, pos_weight=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction=\"none\")\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce = self.bce(logits, targets)  # [B,num_dims]\n",
        "        p = torch.sigmoid(logits)\n",
        "        p_t = p * targets + (1 - p) * (1 - targets)\n",
        "        focal = (1 - p_t).pow(self.gamma)\n",
        "        return (focal * bce).mean()\n",
        "\n",
        "loss_dim = FocalBCEWithLogitsLoss(gamma=2.0, pos_weight=pos_weight_dim)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
        "\n",
        "EPOCHS = 20\n",
        "print(\"Entrenando multi-head...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "\n",
        "    for b in tqdm(train_dl, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        ids = b[\"input_ids\"].to(DEVICE)\n",
        "        att = b[\"attention_mask\"].to(DEVICE)\n",
        "        y_a = b[\"y_area\"].to(DEVICE)         # [B]\n",
        "        y_d = b[\"y_dim\"].to(DEVICE)          # [B,4]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_a, logits_d = model(ids, att)\n",
        "\n",
        "        la = loss_area(logits_a, y_a)\n",
        "        ld = loss_dim(logits_d, y_d)\n",
        "\n",
        "        loss = la + 1.0 * ld  # ✅ peso 1:1 estable (no invento nada)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch} | Loss: {total/len(train_dl):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNkYklC0jA98",
        "outputId": "98ce893a-3ab9-4443-a785-9ee12550d89a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando multi-head...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 25/25 [00:00<00:00, 25.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 2.3214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 25/25 [00:00<00:00, 26.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 2.2799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 25/25 [00:00<00:00, 28.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 2.2583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 25/25 [00:00<00:00, 28.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss: 2.2128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 25/25 [00:00<00:00, 28.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss: 2.1139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 25/25 [00:00<00:00, 28.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Loss: 1.9823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 25/25 [00:00<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Loss: 1.8984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 25/25 [00:00<00:00, 28.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Loss: 1.8216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 25/25 [00:00<00:00, 28.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Loss: 1.7605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 25/25 [00:00<00:00, 28.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 1.6469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 25/25 [00:00<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Loss: 1.4764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 25/25 [00:00<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Loss: 1.3949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 25/25 [00:00<00:00, 28.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Loss: 1.2678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 25/25 [00:00<00:00, 27.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Loss: 1.2860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 25/25 [00:00<00:00, 26.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Loss: 1.1833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 25/25 [00:00<00:00, 26.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Loss: 1.1085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 25/25 [00:00<00:00, 26.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Loss: 1.0445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 25/25 [00:00<00:00, 25.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Loss: 0.9559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 25/25 [00:00<00:00, 28.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Loss: 0.9167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 25/25 [00:00<00:00, 25.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Loss: 0.8602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 9: Calibración threshold para DIM (solo SABER/HACER) + export calibration.json\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, classification_report\n",
        "\n",
        "def eval_dim_threshold(th, top_k=2):\n",
        "    model.eval()\n",
        "    all_probs, all_true = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for b in val_dl:\n",
        "            ids = b[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
        "            att = b[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
        "            y_true = b[\"y_dim\"].cpu().numpy()  # [N,4]\n",
        "\n",
        "            _, logits_d = model(ids, att)\n",
        "            probs_d = torch.sigmoid(logits_d).cpu().numpy()  # [N,4]\n",
        "\n",
        "            all_probs.append(probs_d)\n",
        "            all_true.append(y_true)\n",
        "\n",
        "    y_prob = np.vstack(all_probs)\n",
        "    y_true = np.vstack(all_true)\n",
        "\n",
        "    # ✅ umbral SOLO para SABER/HACER\n",
        "    y_pred = np.zeros_like(y_true)\n",
        "    y_pred[:, 0] = (y_prob[:, 0] >= th).astype(int)  # SABER\n",
        "    y_pred[:, 1] = (y_prob[:, 1] >= th).astype(int)  # HACER\n",
        "\n",
        "    # ✅ fuerza mínimo 1 de las dos (si ninguna activó)\n",
        "    for i in range(y_pred.shape[0]):\n",
        "        if (y_pred[i, 0] + y_pred[i, 1]) == 0:\n",
        "            j = int(np.argmax(y_prob[i, :2]))  # solo entre saber/hacer\n",
        "            y_pred[i, j] = 1\n",
        "\n",
        "    # Métricas SOLO sobre saber/hacer\n",
        "    y_true2 = y_true[:, :2]\n",
        "    y_pred2 = y_pred[:, :2]\n",
        "\n",
        "    return {\n",
        "        \"th\": float(th),\n",
        "        \"top_k\": int(top_k),\n",
        "        \"f1_micro\": float(f1_score(y_true2, y_pred2, average=\"micro\", zero_division=0)),\n",
        "        \"f1_macro\": float(f1_score(y_true2, y_pred2, average=\"macro\", zero_division=0)),\n",
        "        \"precision_macro\": float(precision_score(y_true2, y_pred2, average=\"macro\", zero_division=0)),\n",
        "        \"recall_macro\": float(recall_score(y_true2, y_pred2, average=\"macro\", zero_division=0)),\n",
        "        \"hamming\": float(hamming_loss(y_true2, y_pred2)),\n",
        "        \"avg_dims_per_text\": float(y_pred2.sum(axis=1).mean()),\n",
        "        \"report\": classification_report(y_true2, y_pred2, target_names=DIMENSIONES[:2], zero_division=0)\n",
        "    }\n",
        "\n",
        "best = None\n",
        "for th in np.arange(0.20, 0.61, 0.05):\n",
        "    m = eval_dim_threshold(float(th), top_k=2)\n",
        "    print({k: m[k] for k in [\"th\",\"f1_micro\",\"f1_macro\",\"hamming\",\"avg_dims_per_text\"]})\n",
        "    if best is None or m[\"f1_micro\"] > best[\"f1_micro\"]:\n",
        "        best = m\n",
        "\n",
        "print(\"\\n✅ BEST:\", {k: best[k] for k in [\"th\",\"f1_micro\",\"f1_macro\",\"hamming\",\"avg_dims_per_text\"]})\n",
        "print(\"\\nReporte calibración SABER/HACER:\\n\", best[\"report\"])\n",
        "\n",
        "with open(\"calibration.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"dim_threshold\": best[\"th\"],\n",
        "        \"dim_top_k\": 2,\n",
        "        \"areas\": AREAS,\n",
        "        \"dims\": DIMENSIONES\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Exportado: calibration.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZUCymBxjDUz",
        "outputId": "6aa3684a-7ff8-42a7-b706-f7b9d9376b30"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'th': 0.2, 'f1_micro': 0.6508474576271186, 'f1_macro': 0.6347833164730643, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 2.0}\n",
            "{'th': 0.25, 'f1_micro': 0.6508474576271186, 'f1_macro': 0.6347833164730643, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 2.0}\n",
            "{'th': 0.3, 'f1_micro': 0.6508474576271186, 'f1_macro': 0.6347833164730643, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 2.0}\n",
            "{'th': 0.35, 'f1_micro': 0.6508474576271186, 'f1_macro': 0.6347833164730643, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 2.0}\n",
            "{'th': 0.39999999999999997, 'f1_micro': 0.6508474576271186, 'f1_macro': 0.6347833164730643, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 2.0}\n",
            "{'th': 0.44999999999999996, 'f1_micro': 0.6411149825783972, 'f1_macro': 0.6231884057971014, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 1.9195979833602905}\n",
            "{'th': 0.49999999999999994, 'f1_micro': 0.5818181818181818, 'f1_macro': 0.5749077336891928, 'hamming': 0.4623115577889447, 'avg_dims_per_text': 1.246231198310852}\n",
            "{'th': 0.5499999999999999, 'f1_micro': 0.5370843989769821, 'f1_macro': 0.5317018909899889, 'hamming': 0.4547738693467337, 'avg_dims_per_text': 1.0}\n",
            "{'th': 0.5999999999999999, 'f1_micro': 0.5370843989769821, 'f1_macro': 0.5317018909899889, 'hamming': 0.4547738693467337, 'avg_dims_per_text': 1.0}\n",
            "\n",
            "✅ BEST: {'th': 0.2, 'f1_micro': 0.6508474576271186, 'f1_macro': 0.6347833164730643, 'hamming': 0.5175879396984925, 'avg_dims_per_text': 2.0}\n",
            "\n",
            "Reporte calibración SABER/HACER:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       SABER       0.64      1.00      0.78       128\n",
            "       HACER       0.32      1.00      0.49        64\n",
            "\n",
            "   micro avg       0.48      1.00      0.65       192\n",
            "   macro avg       0.48      1.00      0.63       192\n",
            "weighted avg       0.54      1.00      0.68       192\n",
            " samples avg       0.48      0.89      0.62       192\n",
            "\n",
            "✅ Exportado: calibration.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 10: Inferencia FINAL (robusta)\n",
        "# - Área: reglas (porque en tus PDFs hay encabezados como \"FÍSICA\", \"CIENCIAS SOCIALES\", etc.)\n",
        "# - Dim: LSTM\n",
        "# - SER/DECIDIR: solo si keywords o prob muy alta (gating)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "with open(\"calibration.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    calib = json.load(f)\n",
        "\n",
        "DIM_TH = float(calib[\"dim_threshold\"])\n",
        "# MAX_DIMS_OUT = int(calib[\"dim_top_k\"])\n",
        "MAX_DIMS_OUT = 4\n",
        "\n",
        "\n",
        "# thresholds más estrictos para raras\n",
        "TH_SER = 0.65\n",
        "TH_DECIDIR = 0.75\n",
        "\n",
        "SER_KWS = [\"valorar\",\"respeto\",\"convivir\",\"solidaridad\",\"ético\",\"ética\",\"actitud\",\"equidad\",\"dignidad\",\"inclusión\",\"responsable\"]\n",
        "DEC_KWS = [\"decidir\",\"elegir\",\"proponer\",\"planificar\",\"priorizar\",\"comprometer\",\"promover\",\"argumentar\",\"evaluar\",\"seleccionar\"]\n",
        "\n",
        "def has_kw(text, kws):\n",
        "    t = text.lower()\n",
        "    return any(k in t for k in kws)\n",
        "\n",
        "def rule_area(text: str):\n",
        "    t = text.upper()\n",
        "    # 1) prioridad por encabezados del PDF\n",
        "    for a in AREAS:\n",
        "        if a.replace(\"_\", \" \") in t:\n",
        "            return a\n",
        "    # 2) fallback a tus reglas existentes (CELDA 6)\n",
        "    areas = infer_areas(text)\n",
        "    return areas[0] if areas else \"CIENCIAS_SOCIALES\"\n",
        "\n",
        "def pick_dims(text, prob_d):\n",
        "    # prob_d en orden DIMENSIONES = [\"SABER\",\"HACER\",\"SER\",\"DECIDIR\"]\n",
        "    out = []\n",
        "\n",
        "    # SABER/HACER con threshold calibrado\n",
        "    if prob_d[0] >= DIM_TH: out.append(0)\n",
        "    if prob_d[1] >= DIM_TH: out.append(1)\n",
        "\n",
        "    # SER/DECIDIR con gating (keywords o confianza muy alta)\n",
        "    if has_kw(text, SER_KWS) or prob_d[2] >= TH_SER:\n",
        "        out.append(2)\n",
        "    if has_kw(text, DEC_KWS) or prob_d[3] >= TH_DECIDIR:\n",
        "        out.append(3)\n",
        "\n",
        "    if len(out) == 0:\n",
        "        out = [int(np.argmax(prob_d[:2]))]  # mínimo saber/hacer\n",
        "\n",
        "    # máximo K dims\n",
        "    out = sorted(set(out), key=lambda i: prob_d[i], reverse=True)[:MAX_DIMS_OUT]\n",
        "    return out\n",
        "\n",
        "def predict_final(text: str):\n",
        "    model.eval()\n",
        "\n",
        "    area = rule_area(text)\n",
        "\n",
        "    enc = tokenizer(text, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    att = enc[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, logits_d = model(ids, att)\n",
        "        prob_d = torch.sigmoid(logits_d).cpu().numpy()[0]  # [4]\n",
        "\n",
        "    dims_idx = pick_dims(text, prob_d)\n",
        "    dims = [DIMENSIONES[i] for i in dims_idx]\n",
        "\n",
        "    labels = [{\"label\": f\"{area}__{d}\", \"score\": float(prob_d[DIMENSIONES.index(d)])} for d in dims]\n",
        "    return {\"area\": area, \"labels\": labels}\n",
        "\n",
        "texto = input(\"Pega aquí un texto:\\n\")\n",
        "print(json.dumps(predict_final(texto), ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1dVATOmjIis",
        "outputId": "0e2877e1-37d0-45b1-8ea2-364c96f83c56"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pega aquí un texto:\n",
            "Para tu prueba de modelo, he redactado un texto educativo sobre el Cuidado de la Madre Tierra y la Biodiversidad en Bolivia, integrando las dimensiones de la Ser y el Decidir.  En el modelo educativo boliviano, estas dimensiones son fundamentales: el Ser se enfoca en los valores y la identidad, mientras que el Decidir se orienta a la capacidad de actuar con conciencia crítica y transformar la realidad.  🍃 La Biodiversidad y nuestra Responsabilidad con la Madre Tierra Bolivia es uno de los países con mayor biodiversidad en el mundo, albergando desde las cumbres nevadas de los Andes hasta las llanuras amazónicas. Sin embargo, proteger esta riqueza no solo requiere conocimientos científicos, sino una transformación profunda en nuestra forma de convivir con el entorno.  La Dimensión del Ser: Valores y Conciencia En el desarrollo de esta unidad, fortalecemos el Ser mediante la recuperación de valores sociocomunitarios como la complementariedad y el respeto hacia todos los seres vivos. No vemos a la naturaleza como un simple recurso, sino como la Madre Tierra (Pachamama), con quien mantenemos una relación de interdependencia.  Identidad Cultural: Reconocemos nuestras raíces y la sabiduría de los pueblos indígenas que han vivido en armonía con la tierra por siglos.  Ética Comunitaria: Fomentamos la honestidad y la solidaridad al compartir los beneficios de un medio ambiente sano.  La Dimensión del Decidir: Acción y Transformación El aprendizaje no termina en el aula; se manifiesta en el Decidir. Esta dimensión se observa cuando el estudiante asume una postura política y social frente a la crisis climática. Imprime una voluntad de transformación social a través de acciones concretas que impactan en la comunidad.  Postura Crítica: Capacidad de cuestionar los modelos de consumo que dañan el ecosistema local.  Impacto Social: Voluntad política de organizar campañas de reforestación o proponer leyes municipales que protejan las cuencas de agua.  Autonomía: Tomar la decisión personal de reducir el uso de plásticos y promover el reciclaje, influyendo positivamente en su familia y entorno.\n",
            "{\n",
            "  \"area\": \"VALORES_ESPIRITUALIDAD_RELIGIONES\",\n",
            "  \"labels\": [\n",
            "    {\n",
            "      \"label\": \"VALORES_ESPIRITUALIDAD_RELIGIONES__DECIDIR\",\n",
            "      \"score\": 0.5526168942451477\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"VALORES_ESPIRITUALIDAD_RELIGIONES__HACER\",\n",
            "      \"score\": 0.5257401466369629\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"VALORES_ESPIRITUALIDAD_RELIGIONES__SER\",\n",
            "      \"score\": 0.5163319706916809\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"VALORES_ESPIRITUALIDAD_RELIGIONES__SABER\",\n",
            "      \"score\": 0.46922892332077026\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 11: Exporta modelo + labels + tokenizer + config (MULTI-HEAD REAL)\n",
        "\n",
        "EXPORT_DIR = \"/content/model_export_red_1_LSTM\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(EXPORT_DIR, \"pedagogical_lstm.pt\")\n",
        "labels_path = os.path.join(EXPORT_DIR, \"labels.json\")\n",
        "tok_path   = os.path.join(EXPORT_DIR, \"tokenizer_name.txt\")\n",
        "cfg_path   = os.path.join(EXPORT_DIR, \"config.json\")\n",
        "\n",
        "# pesos\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# labels (combinaciones AREA__DIM)\n",
        "with open(labels_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(LABELS, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# tokenizer\n",
        "with open(tok_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(TOKENIZER_NAME)\n",
        "\n",
        "# config (usa tu mejor threshold real)\n",
        "cfg = {\n",
        "    \"model_type\": \"lstm_multihead\",\n",
        "    \"vocab_size\": int(tokenizer.vocab_size),\n",
        "    \"pad_token_id\": int(tokenizer.pad_token_id),\n",
        "    \"emb_dim\": int(EMB_DIM),\n",
        "    \"hidden_dim\": int(HIDDEN),\n",
        "    \"max_len\": int(MAX_LEN),\n",
        "\n",
        "    # clases\n",
        "    \"areas\": AREAS,\n",
        "    \"dims\": DIMENSIONES,\n",
        "    \"labels\": LABELS,\n",
        "\n",
        "    # calibración (tu BEST)\n",
        "    \"threshold_default\": 0.20,\n",
        "    \"top_areas\": 3,\n",
        "    \"top_k_combos\": 8,\n",
        "    \"max_active_combos\": 12\n",
        "}\n",
        "\n",
        "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Export listo en:\", EXPORT_DIR)\n",
        "print(\"Archivos:\", os.listdir(EXPORT_DIR))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjyiLCD-v6I1",
        "outputId": "c4e36270-54db-4b04-9f10-9276d11aae14"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Export listo en: /content/model_export_red_1_LSTM\n",
            "Archivos: ['labels.json', 'pedagogical_lstm.pt', 'config.json', 'tokenizer_name.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CELDA 12: Loader + inferencia FINAL (MULTI-HEAD, igual a tu entrenamiento)\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ---------- Modelo MULTI-HEAD ----------\n",
        "class PedagogicalLSTMMultiHead(nn.Module):\n",
        "    def __init__(self, vocab_size, pad_token_id, emb_dim, hidden_dim, num_areas, num_dims):\n",
        "        super().__init__()\n",
        "        # IMPORTANTE: nombre \"embedding\" para que coincida con tu state_dict\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_token_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # IMPORTANTE: nombres fc_area y fc_dim para que coincidan con tu state_dict\n",
        "        self.fc_area = nn.Linear(hidden_dim * 2, num_areas)\n",
        "        self.fc_dim  = nn.Linear(hidden_dim * 2, num_dims)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids)          # [B,T,E]\n",
        "        out, _ = self.lstm(x)                  # [B,T,2H]\n",
        "\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        out = out * mask\n",
        "        pooled = out.sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)  # masked mean\n",
        "        pooled = self.dropout(pooled)\n",
        "\n",
        "        logits_area = self.fc_area(pooled)     # [B,num_areas]\n",
        "        logits_dim  = self.fc_dim(pooled)      # [B,num_dims]\n",
        "        return logits_area, logits_dim\n",
        "\n",
        "# ---------- Loader ----------\n",
        "def load_classifier(export_dir: str, device: str = \"cpu\"):\n",
        "    cfg = json.load(open(os.path.join(export_dir, \"config.json\"), encoding=\"utf-8\"))\n",
        "    tok_name = open(os.path.join(export_dir, \"tokenizer_name.txt\"), encoding=\"utf-8\").read().strip()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tok_name)\n",
        "\n",
        "    areas = cfg[\"areas\"]\n",
        "    dims  = cfg[\"dims\"]\n",
        "\n",
        "    model = PedagogicalLSTMMultiHead(\n",
        "        vocab_size=cfg[\"vocab_size\"],\n",
        "        pad_token_id=cfg[\"pad_token_id\"],\n",
        "        emb_dim=cfg[\"emb_dim\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_areas=len(areas),\n",
        "        num_dims=len(dims)\n",
        "    )\n",
        "\n",
        "    sd = torch.load(os.path.join(export_dir, \"pedagogical_lstm.pt\"), map_location=device)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer, cfg\n",
        "\n",
        "# ---------- Utilidades ----------\n",
        "def build_combo_scores(area_probs, dim_probs, areas, dims):\n",
        "    \"\"\"\n",
        "    Construye scores para todas las combinaciones AREA__DIM como producto:\n",
        "    score(area__dim) = area_prob * dim_prob\n",
        "    (estable y consistente para pasar a Red 2)\n",
        "    \"\"\"\n",
        "    combos = []\n",
        "    for ai, a in enumerate(areas):\n",
        "        for di, d in enumerate(dims):\n",
        "            combos.append((f\"{a}__{d}\", float(area_probs[ai] * dim_probs[di])))\n",
        "    combos.sort(key=lambda x: -x[1])\n",
        "    return combos\n",
        "\n",
        "# ---------- Inferencia FINAL ----------\n",
        "def classify_text(model, tokenizer, cfg, text: str, threshold: float | None = None, device: str = \"cpu\"):\n",
        "    thr = cfg[\"threshold_default\"] if threshold is None else float(threshold)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=cfg[\"max_len\"],\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    ids = enc[\"input_ids\"].to(device)\n",
        "    att = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_area, logits_dim = model(ids, att)\n",
        "        #area_probs = torch.sigmoid(logits_area).cpu().numpy()[0]\n",
        "        area_probs = torch.softmax(logits_area, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "\n",
        "        dim_probs  = torch.sigmoid(logits_dim).cpu().numpy()[0]\n",
        "\n",
        "    areas = cfg[\"areas\"]\n",
        "    dims  = cfg[\"dims\"]\n",
        "\n",
        "    # TOP 3 áreas\n",
        "    area_rank = sorted(\n",
        "        [{\"area\": areas[i], \"score\": float(area_probs[i])} for i in range(len(areas))],\n",
        "        key=lambda x: -x[\"score\"]\n",
        "    )\n",
        "    areas_top3 = area_rank[:cfg[\"top_areas\"]]\n",
        "\n",
        "    # dims probs (4)\n",
        "    dims_out = {dims[i]: float(dim_probs[i]) for i in range(len(dims))}\n",
        "\n",
        "    # combinaciones (AREA__DIM) como producto\n",
        "    combo_scores = build_combo_scores(area_probs, dim_probs, areas, dims)\n",
        "\n",
        "    # TOP combos\n",
        "    top_combos = [{\"label\": c[0], \"score\": c[1]} for c in combo_scores[:cfg[\"top_k_combos\"]]]\n",
        "\n",
        "    # combos activas por threshold (limitadas)\n",
        "    active = [{\"label\": c[0], \"score\": c[1]} for c in combo_scores if c[1] >= thr]\n",
        "    if not active:\n",
        "        active = [{\"label\": combo_scores[0][0], \"score\": combo_scores[0][1]}]\n",
        "    active = active[:cfg[\"max_active_combos\"]]\n",
        "\n",
        "    return {\n",
        "        \"threshold\": thr,\n",
        "        \"areas_top\": areas_top3,\n",
        "        \"dims_probs\": dims_out,\n",
        "        \"combo_top\": top_combos,\n",
        "        \"active\": active\n",
        "    }\n",
        "\n",
        "# ---------- Mini test ----------\n",
        "export_dir = \"/content/model_export_red_1_LSTM\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clf_model, clf_tok, clf_cfg = load_classifier(export_dir, device=device)\n",
        "print(\"✅ Loader OK | areas:\", len(clf_cfg[\"areas\"]), \"| dims:\", len(clf_cfg[\"dims\"]))\n",
        "\n",
        "test_text = \"Movimiento parabólico, aplicar fórmulas y explicar el procedimiento.\"\n",
        "out = classify_text(clf_model, clf_tok, clf_cfg, test_text, device=device)\n",
        "print(json.dumps(out, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVlEQVxXwRi7",
        "outputId": "34445e7e-afe0-4da6-edcb-b652650ebace"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loader OK | areas: 8 | dims: 4\n",
            "{\n",
            "  \"threshold\": 0.2,\n",
            "  \"areas_top\": [\n",
            "    {\n",
            "      \"area\": \"FISICA\",\n",
            "      \"score\": 0.8475247621536255\n",
            "    },\n",
            "    {\n",
            "      \"area\": \"MATEMATICA\",\n",
            "      \"score\": 0.09015604853630066\n",
            "    },\n",
            "    {\n",
            "      \"area\": \"CIENCIAS_NATURALES_BIOLOGIA\",\n",
            "      \"score\": 0.034537892788648605\n",
            "    }\n",
            "  ],\n",
            "  \"dims_probs\": {\n",
            "    \"SABER\": 0.5325000882148743,\n",
            "    \"HACER\": 0.5034955739974976,\n",
            "    \"SER\": 0.4422486126422882,\n",
            "    \"DECIDIR\": 0.2825419306755066\n",
            "  },\n",
            "  \"combo_top\": [\n",
            "    {\n",
            "      \"label\": \"FISICA__SABER\",\n",
            "      \"score\": 0.4513069987297058\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"FISICA__HACER\",\n",
            "      \"score\": 0.42672497034072876\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"FISICA__SER\",\n",
            "      \"score\": 0.3748166561126709\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"FISICA__DECIDIR\",\n",
            "      \"score\": 0.239461287856102\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"MATEMATICA__SABER\",\n",
            "      \"score\": 0.04800810292363167\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"MATEMATICA__HACER\",\n",
            "      \"score\": 0.04539317265152931\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"MATEMATICA__SER\",\n",
            "      \"score\": 0.03987138718366623\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"MATEMATICA__DECIDIR\",\n",
            "      \"score\": 0.025472864508628845\n",
            "    }\n",
            "  ],\n",
            "  \"active\": [\n",
            "    {\n",
            "      \"label\": \"FISICA__SABER\",\n",
            "      \"score\": 0.4513069987297058\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"FISICA__HACER\",\n",
            "      \"score\": 0.42672497034072876\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"FISICA__SER\",\n",
            "      \"score\": 0.3748166561126709\n",
            "    },\n",
            "    {\n",
            "      \"label\": \"FISICA__DECIDIR\",\n",
            "      \"score\": 0.239461287856102\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}